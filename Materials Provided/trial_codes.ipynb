{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming df_train and 'winner' column exist in your dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Select numerical columns\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m numerical_cols \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Exclude the target column from numerical_cols if needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m numerical_cols:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 57.2 waala model hai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming df_train and 'winner' column exist in your dataset\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Exclude the target column from numerical_cols if needed\n",
    "if 'winner' in numerical_cols:\n",
    "    numerical_cols = numerical_cols.drop('winner')\n",
    "\n",
    "# Exclude additional columns if necessary\n",
    "numerical_cols = numerical_cols.drop(['team1_id', 'match id', 'team2_id', 'ground_id'])\n",
    "\n",
    "# Split the data into train, test, and validation sets with stratification\n",
    "X = df_train[top_features_cumulative]\n",
    "y = df_train['winner']\n",
    "\n",
    "# Split the data with stratification\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Initialize the models with default parameters\n",
    "catboost_model = CatBoostClassifier(verbose=False, learning_rate=0.1)\n",
    "xgb_model = XGBClassifier(verbosity=0, learning_rate=0.1)\n",
    "lgbm_model = LGBMClassifier(learning_rate=0.1)\n",
    "gbm_model = GradientBoostingClassifier(learning_rate=0.1)\n",
    "\n",
    "# Train the models on the training set\n",
    "catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "lgbm_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "catboost_val_pred = catboost_model.predict_proba(X_val)[:, 1]\n",
    "xgb_val_pred = xgb_model.predict_proba(X_val)[:, 1]\n",
    "lgbm_val_pred = lgbm_model.predict_proba(X_val)[:, 1]\n",
    "gbm_val_pred = gbm_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Stack predictions as new features for validation set\n",
    "val_preds = np.column_stack((catboost_val_pred, xgb_val_pred, lgbm_val_pred, gbm_val_pred))\n",
    "\n",
    "# Train the meta-model on the stacked predictions\n",
    "meta_model = CatBoostClassifier(verbose=False, learning_rate=0.1)\n",
    "meta_model.fit(val_preds, y_val)\n",
    "\n",
    "# Perform cross-validation on the validation set and report min, max, and average scores\n",
    "cv_scores = cross_val_score(meta_model, val_preds, y_val, cv=5, scoring='accuracy')\n",
    "print(f\"CV Scores - Avg: {cv_scores.mean()}, Min: {cv_scores.min()}, Max: {cv_scores.max()}\")\n",
    "\n",
    "# Get predictions for the test set\n",
    "catboost_test_pred = catboost_model.predict_proba(X_test)[:, 1]\n",
    "xgb_test_pred = xgb_model.predict_proba(X_test)[:, 1]\n",
    "lgbm_test_pred = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "gbm_test_pred = gbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Stack predictions as new features for test set\n",
    "test_preds = np.column_stack((catboost_test_pred, xgb_test_pred, lgbm_test_pred, gbm_test_pred))\n",
    "\n",
    "# Predict on the test set using the meta-model\n",
    "y_pred_test = meta_model.predict(test_preds)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Get feature importances\n",
    "catboost_importances = catboost_model.get_feature_importance()\n",
    "xgb_importances = xgb_model.feature_importances_\n",
    "lgbm_importances = lgbm_model.feature_importances_\n",
    "gbm_importances = gbm_model.feature_importances_\n",
    "\n",
    "# Combine feature importances into a DataFrame\n",
    "ft_imp_cat = pd.DataFrame({'Feature': X.columns, 'Importance': catboost_importances})\n",
    "ft_imp_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances})\n",
    "ft_imp_lgbm = pd.DataFrame({'Feature': X.columns, 'Importance': lgbm_importances})\n",
    "ft_imp_gbm = pd.DataFrame({'Feature': X.columns, 'Importance': gbm_importances})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "file_path = \"/content/processed_train_f (2).csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "irrelevant_columns = [\n",
    "    'match id', 'team1', 'team1_id', 'team1_roster_ids',\n",
    "    'team2', 'team2_id', 'team2_roster_ids', 'venue', 'city',\n",
    "    'match_dt', 'series_name', 'season'\n",
    "]\n",
    "data_cleaned = data.drop(columns=irrelevant_columns)\n",
    "\n",
    "# Select only numeric columns\n",
    "data_numeric = data_cleaned.select_dtypes(include=['number'])\n",
    "\n",
    "# Fill missing values with the mean of their respective columns\n",
    "data_filled = data_numeric.fillna(data_numeric.mean())\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data_filled.drop(columns=['winner'])\n",
    "y = data_filled['winner']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 100)\n",
    "        self.fc2 = nn.Linear(100, 35)\n",
    "        self.fc3 = nn.Linear(35, 2)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        embeddings = x  # Extract embeddings here\n",
    "        x = self.fc3(x)\n",
    "        return x, embeddings\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _ = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Extract embeddings from the trained model\n",
    "model.eval()\n",
    "_, embeddings = model(X_tensor)\n",
    "\n",
    "embeddings = embeddings.detach().numpy()\n",
    "\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train CatBoost model on the entire dataset embeddings\n",
    "catboost_model = CatBoostClassifier(verbose=0)\n",
    "catboost_model.fit(embeddings, y)\n",
    "\n",
    "# Predict on the entire dataset\n",
    "y_pred_catboost = catboost_model.predict(embeddings)\n",
    "\n",
    "# Evaluate the CatBoost model\n",
    "accuracy_catboost = accuracy_score(y, y_pred_catboost)\n",
    "report_catboost = classification_report(y, y_pred_catboost)\n",
    "\n",
    "# Perform cross-validation on the entire dataset\n",
    "cv_scores = cross_val_score(catboost_model, embeddings, y, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"CatBoost Accuracy: {accuracy_catboost}\")\n",
    "print(f\"CatBoost Classification Report:\\n{report_catboost}\")\n",
    "print(f\"Cross-Validation Scores - Avg: {cv_scores.mean()}, Min: {cv_scores.min()}, Max: {cv_scores.max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
